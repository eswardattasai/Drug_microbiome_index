{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "file_path = \"/content/pan_meta.csv\"  # file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "numerical_cols = [\"AGE  in years\", \"BMI\"]\n",
    "X = df[numerical_cols]\n",
    "\n",
    "X_standardized = (X - X.mean()) / X.std()\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_standardized)\n",
    "df_pca = pd.DataFrame(principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# Labellin\n",
    "categorical_vars = [\"Gender\", \"Life style pattern\", \"Obese-Non Obese\"]\n",
    "df_pca[categorical_vars] = df[categorical_vars]\n",
    "\n",
    "# PCA results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, category in enumerate(categorical_vars):\n",
    "    sns.scatterplot(ax=axes[i], x=\"PC1\", y=\"PC2\", hue=df_pca[category], alpha=0.7, palette=\"coolwarm\", data=df_pca)\n",
    "    axes[i].set_title(category)\n",
    "    axes[i].set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "    axes[i].set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "#File Path\n",
    "file_path = '/content/pan_otutab.csv'\n",
    "csv_data = pd.read_csv(file_path)\n",
    "\n",
    "csv_data.head()\n",
    "\n",
    "     \n",
    "OTU_ID\tSubject-1067\tSubject-1090\tSubject-2032\tSubject-2065\tSubject-3026\tSubject-3061\tSubject-4010\tSubject-4027\tSubject-5003\t...\tSubject-13029\tSubject-1034\tSubject-11023\tSubject-13030\tSubject-1035\tSubject-1006\tSubject-1013\tSubject-1014\tSubject-1007\tSubject-1036\n",
    "0\totu000001\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "1\totu000002\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "2\totu000003\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "3\totu000004\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "4\totu000005\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t...\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\t0.0\n",
    "5 rows Ã— 1005 columns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#Data Scalin\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(csv_data)\n",
    "\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
